\documentclass[12pt, titlepage]{article}

% Packages

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage[round]{natbib}
\usepackage[usenames, dvipsnames]{color}
\usepackage{tikz}

% Setup

\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=ForestGreen,
    linkcolor=MidnightBlue,
    urlcolor=blue
}

\lstset{
	basicstyle=\ttfamily\footnotesize
}

% Custom Commands

\newcounter{funCounter}
\newcounter{nonCounter}
\newcounter{pocCounter}

\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0em}}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0em}}p{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0em}}p{#1}}

\newcommand\testTitle{NULL}
\newcommand\testCounter{NULL}

% Arguments
%    1 = {F, N, P} for functional, non-functional, and PoC respectively
%	 2 = Test name
%    3 = {Static, Dynamic}
%    4 = {Manual, Automatic}
%    5 = {Black, White}
%    6 = Initial state
%    7 = Input
%    8 = Output
%    9 = How the test will be performed
\newcommand{\test}[9]{	
	\begin{table}[H]
		\if#1F
			\stepcounter{funCounter}
			\renewcommand\testTitle{Functional }
			\renewcommand\testCounter{\thefunCounter} 
		\else
			\if#1N
				\stepcounter{nonCounter}
				\renewcommand\testTitle{Non-Functional }
				\renewcommand\testCounter{\thenonCounter} 
			\else
				\stepcounter{pocCounter}
				\renewcommand\testTitle{PoC }
				\renewcommand\testCounter{\thepocCounter} 
			\fi
		\fi
		
		\centering
		\def\arraystretch{1.6}
		\begin{tabular}{| R{7em} L{28em} |}
			\bottomrule
			\multicolumn{2}{| c |}{\textbf{#2} - \testTitle Test \# \testCounter} \\
			\hline
			\textit{Type:} & #3 / #4 / #5 Box\\
			\textit{Initial State:} & #6 \\
			\textit{Input:} & #7 \\
			\textit{Output:} & #8 \\
			\textit{Execution:} & #9 \\
			\toprule
		\end{tabular}
	\end{table}
	\smallskip
}

% Title Page Elements

\title{SE 3XA3: Test Plan\\Rogue Reborn}

\author{Group \#6, Team Rogue++\\\\
	\begin{tabular} {l r}
		Ian Prins & prinsij \\
		Mikhail Andrenkov & andrem5 \\
		Or Almog & almogo
	\end{tabular}
}

\date{Due Monday, October 31\textsuperscript{st}, 2016}

\input{../Comments}

\begin{document}

\maketitle

% Report outline

\pagenumbering{roman}
\tableofcontents
\listoftables
\listoffigures

% Revision Table

\begin{table}[bp!]
	\caption{\bf Revision History}
	\bigskip
	\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
		\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
		\midrule
		10/21/16 & 0.0 & Initial Setup\\
		10/24/16 & 0.1 & Added Unit Testing and Usability Survey \\
		10/24/16 & 0.2 & Added Most of Section 2 \\
		10/24/16 & 0.3 & Added Section 1 \\
		10/26/16 & 0.4 & Added PoC tests \\
		10/26/16 & 0.4.1 & Added Test Template \\
		10/30/16 & 0.5 & Added Non-Functional Req. Tests \\
		10/30/16 & 0.5.1 & Added Bibliography \\
		10/31/16 & 0.6 & Switch PoC to Test Template \\
		10/31/16 & 0.7 & Add Name to Test Template \\
		\bottomrule
	\end{tabularx}
\end{table}

\newpage

% Report Content

\pagenumbering{arabic}

\section{General Information}
\label{section1}

	\subsection{Purpose}
		The purpose of this document is to explore the verification process that will be applied to the Rogue Reborn project.  After reviewing the document, the reader should understand the strategy, focus, and motivation behind the efforts of the Rogue++ testing team.   

	\subsection{Scope}
		This report will encompass all technical aspects of the testing environment and implementation plan, as well as other elements in the domain of team coordination and project deadlines.  The document will also strive to be comprehensive by providing context behind critical decisions, motivating the inclusion of particular features by referring to the existing \textit{Rogue} implementation, and offering a large variety of tests for various purposes and hierarchical units.  Aside from the implementation, the report will also discuss a relevant component from the requirements elicitation process.

	\subsection{Acronyms, Abbreviations, and States}
		
		\begin{table}[H]
			\centering
			\caption{\textbf{Table of Abbreviations and Acronyms}}
			\label{TableAbbreviations}
			\bigskip
			\begin{tabularx}{\textwidth}{p{3cm}X}
				\toprule
				\textbf{Abbreviation} & \textbf{Definition} \\
				\midrule
				GUI & Graphical User Interface\\
				PoC & Proof of Concept\\
				\bottomrule
			\end{tabularx}
		\end{table}

		\begin{table}[H]
			\centering
			\caption{\textbf{Table of Definitions}}
			\label{TableDefinitions}
			\bigskip
			\begin{tabularx}{\textwidth}{p{3.5cm}X}
				\toprule
				\textbf{Term} & \textbf{Definition}\\
				\midrule
				\textbf{Boost} & C++ utility library that includes a comprehensive unit testing framework\\
				\textbf{Frame} & An instantaneous ``Snapshot'' of the GUI screen\\
				\textbf{Libtcod} & Graphics library that specializes in emulating a roguelike experience\\
				\textbf{Monochrome Luminance} & The brightness of a given colour (with respect to the average sensitivity of the human eye)\\
				\textbf{Permadeath} & Feature of roguelike games whereby a character death will end the game\\
				\textbf{Roguelike} & Genre of video games characterized by ASCII graphics, procedurally-generated levels, and permadeath\\
				\bottomrule
			\end{tabularx}
		\end{table}	

		\begin{table}[H]
			\centering
			\caption{\textbf{Table of States}}
			\label{StateDefinitions}
			\bigskip
			\begin{tabularx}{\textwidth}{p{3.5cm}X}
				\toprule
				\textbf{State} & \textbf{Definition}\\
				\midrule
				\textbf{Developer State} & The file system state corresponding to the latest source code revision from the Git repository\\
				\textbf{Fresh State} & The file system state corresponding to a ``fresh'' Rogue Reborn installation\\
				\textbf{Gameplay State} & Any application state that reflects the actual gameplay\\
				\textbf{Generic State} & The file system state corresponding to a functional installation of Rogue Reborn\\
				\textbf{High Score State} & Any application state that reflects the top high scores screen\\
				\textbf{Menu State} & Any application state that reflects the opening menu\\
				\textbf{Seasoned State} & The system state corresponding to an installation of Rogue Reborn that already contains several high score records\\
				\bottomrule
			\end{tabularx}
		\end{table}	

	\subsection{Overview of Document}
		The early sections of the report will describe the testing environment and the logistic components of the Rogue Reborn testing effort, including the schedule and work allocation.  Next, a suite of tests will be discussed with respect to the functional requirements, nonfunctional requirements, and proof of concept demonstration.  Upon discussing the relevance of this project to the original \textit{Rogue}, a variety of unit tests will be given followed by a sample usability survey to guage the interest and opinion of the Rogue Reborn game.  A breakdown of the sections is listed below:

		\begin{itemize}
			\item \hyperref[section1]{\S 1} Brief overview of the report contents
			\item \hyperref[section2]{\S 2} Project logistics and the software testing environment
			\item \hyperref[section3]{\S 3} Description of system-level integration tests (based on requirements)
			\item \hyperref[section4]{\S 4} Explanation of test plans that were inspired by the PoC demonstration
			\item \hyperref[section5]{\S 5} Comparison of the existing \textit{Rogue} to the current project in the context of testing
			\item \hyperref[section6]{\S 6} Outline of the module-level unit tests 
			\item \hyperref[section7]{\S 7} Appendix for symbolic parameters and the aforementioned usability survey
		\end{itemize}

\newpage
\section{Plan}
\label{section2}
		
	\subsection{Software Description}

	Initially, the plan for testing involved the usage of a pre-made testing system called Boost. Boost has industry renown and is very well documented. The drawback to using such a profound system is exactly its advantage - it is heavy, globally encompassing, and requires a lot of work to use properly. The Boost library is suitable for projects spanning years, with dedicated testing teams. This is not the present situation. With hardly over a month until the completion of the project, starting to use Boost would be most unwise.\\

	Instead, an alternative solution has been proposed and implemented. Native test cases can be written in C++ to do exactly that which is required. The details of this implementation will be explained in the parts to follow.

	\subsection{Test Team}

	All members of the team will take part in the testing procedure. While Mikhail was given the title of project manager, and Ian C++ expert, Ori was assigned the role of testing expert. Testing will be monitored by Ori, but of course every member of the team will contribute to the testing facilities. It would be desirable for the team member who wrote class $C$ to write the unit tests for this class. Due to the dependency-tree-like structure of the project's design, there will be cases where a unit test for one class encompasses a partial system test for another one. This can be extrapolated from the class inheritance diagram.

	\subsection{Automated Testing Approach}

	We have made a very large attempt at automating whatever we could for this project. In the real world, any task that \textit{can} be automated, is automated. The steps we have taken are as follows:

	\begin{itemize}
		\item Set up a GitLab pipeline for the project. The pipeline is programmed to run a series of commands on an external VPS whenever a push is made to the git repository. Each run is documented and its history may be accessed.
		\item Write a special makefile that outputs 2 executables: the first being the actual project, and the second the project's tests. The details will be delved into in the following sub-section.
		\item The team's primary method of communication is Slack, a cross-platform, programmer-friendly chat interface. We hooked up the GitLab project repository to the Slack channel such that whenever a push is made or an issue addressed, a notification is sent. This method makes it far easier to communicate about project-related inquiries.
	\end{itemize}

	\subsection{Testing Tools}

	The special makefile discussed previously utilizes a phenomenon of C++ to perform the necessary steps. First, it places \textit{all} source files into a dedicated folder, distinguishing between program files and test files. This is an absolutely necessary step, as there is an important relationship between \textit{source} and \textit{test} classes. See the diagram below:

	\bigskip
	\bigskip
	\bigskip

	% Venn-diagram
	\begin{center}
		\begin{tikzpicture}[fill=white]
			% left hand
			\scope
			\clip (1,0) circle (1);
			\endscope

			% right hand
			\scope
			\clip (0,0) circle (1);
			\endscope

			% outline
			\draw 	(-1,0) circle (2) (-4,1)  node [text=black,above] {$Source$}
			 		(1,0) circle (2) (4,1)  node [text=black,above] {$Test$};
		\end{tikzpicture}
	\end{center}

	\bigskip
	\bigskip

	As the diagram above depicts, there are classes shared between both final programs. The vast majority of classes fall in the center, required by both the final project and its testing component. The files required by the test which are not required by the source are, obviously, testing-related files. These are the files that contain the test case implementations. At the time of writing, there is actually only one file required by source that is not required by the test, and that is the source program entry (i.e. the file that contains the main() method).

	\bigskip

	The entire procedure of file collection, compilation, and separate linking is handled by the makefile, and is triggered by the "make" command. Then, simply running Test.exe will fire off all of the pre-written tests.

	\bigskip

	There is a plan to implement a python script on the GitLab pipeline that will cause the build to fail if any of the tests do not pass. At the time of writing this document, it is not yet implemented, but note will be made when it does. It should be noted that if a build fails, the pipeline not only reports the failure, but also logs where the failure happened, down to the specific test case. This will hopefully make debugging a more pleasant experience later on.

	\bigskip

	As an extra safety measure, the Rogue++ team will also be utilizing a tool called Valgrind in the testing procedure. Valgrind is a tool that tests the amount of memory a C++ program utilizes, and detects memory allocation errors (such as memory leaks). This is an extremely useful and powerful tool. C++, unlike Java and other high level languages, does not have a built-in garbage collector. This is just one of the reasons why it is so much faster than the rest. A consequence of this, however, is that it is very easy to accidentally leave behind an object in memory, causing a memory leak in the program.\\

	At the time of writing, the entire program occupies 1 MB of memory. This is not much, and even if it was all left behind in a leak, the system would not be too hindered. However, memory leaks represent only a consequence of a larger issue: incorrect code! Using Valgrind, we will be able to detect these kinds of errors, potentially pointing us in the direction of a crucial bugfix.

	\subsection{Testing Schedule}
		
	See Gantt Chart at the following url ... TODO

\newpage
\section{System Test Description}
\label{section3}
	
	\subsection{Tests for Functional Requirements}

		%Dynamic 	Conduct test by running program
		%Static 	Conducted without running program (spell check)
		%~~~
		%Manual 	Actual human being has to decide if result is correct or not
		%Automatic 	Script
		%~~~
		%Black 		Decided on input and output
		%White 		Generate test cases based on code (Use code to make decisions)

		%123
		%initial state
		%input
		%output
		%how it will be performed

		\subsubsection{Basic Mechanics}

			\test{F}{New game start}{Dynamic}{Manual}{Black}{Nothing running.}{A new game is started.}{The program is started.}{Either double-clicking the .exe or via terminal: \textit{./RogueReborn.exe}.}

			\test{F}{Save game}{Dynamic}{Manual}{Black}{Game screen}{Save command is given or save key is pressed.}{A message saying that the game has been saved is shown to the user in the status box.}{A user will have to play the game and trigger the input sequence. This process can be verified to work by the following test.}

			\test{F}{Load game}{Dynamic}{Manual}{Black}{Game screen}{Load command is given or save key is pressed.}{A message saying that the game has been loaded is shown to the user in the status box. The data model (level, player, monsters, etc.) is also updated to reflect the state changes.}{A user will have to play the game and trigger the input sequence to load, and verify that it is in fact the same state that was previously saved.}

			\test{F}{New game starting statistics}{Dynamic}{Automatic}{Black}{Nothing running.}{A new game is started.}{The player has the default starting gear and statistics.}{This feature can be tested by analyzing a save file. In the file is listed everything about the player, meaning the information can be attained from there.}

			\test{F}{Help command}{Dynamic}{Manual}{Black}{Game screen}{The "help" command is given or the "help" key is pressed.}{The user is shown a screen with a list of possible actions and other information}{Players will be given the game with no instructions or guide. The usefulness and accessibility of the help screen will be judged by their performance after having seen the help screen.}

		\subsubsection{Interaction}

			\test{F}{Detailer player information}{Dynamic}{Manual}{Black}{Game screen.}{None.}{Details about the player (such as level, health, known status effects, current depth, etc.) are displayed at the bottom of the screen, in the area known as the "Info bar".}{At random points during the playtest, players will be asked to answer basic questions about their player. To answer these questions, the player will have to refer to the info bar.}

			\test{F}{Environment inspection}{Dynamic}{Manual}{Black}{Game screen.}{The "look" key or command, and then an environment aspect character.}{After the input is supplied, a brief description of the environment aspect is supplied. This can be limited to a word or two (i.e. "This is an Emu").}{Players will be told about the "look" key before starting, and will have to employ it to get to know their surroundings.}

			\test{F}{Pass turn}{Dynamic}{Manual}{Black}{Game screen.}{The player wishes to skip his turn. This is usually the case if an enemy is about to move perpendicularly to the player's pre-determined projectile path, which will place the enemy in the direction of the player's projectile.}{All entities but the player act, performing whatever action their AI has instructed them to perform.}{Players will be asked to skip their turn several times once an enemy is spotted.}

			\test{F}{Trap activation}{Dynamic}{Manual}{Black}{Game screen.}{A dungeon level that can generate traps (This only occurs in the deeper levels).}{A message and effect describing what the trap has done.}{Players will be asked to report any trap they come across and the effect it has bestowed upon them.}

		\subsubsection{The Dungeon}

			\test{F}{Staircase guarantee}{Dynamic}{Automatic}{Black}{Nothing running.}{A randomly generated dungeon (preferably many).}{An assertion that all contain a downwards staircase.}{The algorithm for this is rather straight-forward; it is a simple BFS or DFS touring every passable block in the dungeon.}

			\test{F}{Connectedness \& Reachability}{Dynamic}{Automatic}{White}{Nothing running.}{A randomly generated dungeon (preferably many).}{An assertion that the dungeon is connected and all tile are reachable from one-another.}{Again, another simple algorithm. A BFS or DFS can acquire a list of all passable tiles in the dungeon, which can be compared to the list provided by the source-code. If the two lists match, then the assertion is true.}

			\test{F}{Line of Sight}{Dynamic}{Manual}{Black}{Game screen.}{Player is somewhere in the dungeon that is recognizable (i.e. not hidden), and player is not blind.}{Visibility dependent on surroundings. If in a room, the player should be able to see the entire room. If in a corridor, the player should only be able to see in a 3x3 square centered on the player.}{Players will be asked to assess the visibility standards. This is a bug-prone feature, as many exceptions exist in the realm of "What is the player on?".}

			\test{F}{Amulet of Yendor}{Dynamic}{Automatic}{White}{Nothing running.}{Levels generated with a depth of 26}{A correct assertion that all levels generated contain the amulet somewhere on the level.}{It only takes a double-nested for-loop to make sure that somewhere in the level, on a passable tile, the amulet exists. Any since we already know that every passable tile is reachable, we know that the amulet is as well.}
			\test{F}{Name}{Type}{ManAut}{Color}{InitState}{Input}{Output}{Execution}
			\test{F}{Name}{Type}{ManAut}{Color}{InitState}{Input}{Output}{Execution}
			\test{F}{Name}{Type}{ManAut}{Color}{InitState}{Input}{Output}{Execution}
			\test{F}{Name}{Type}{ManAut}{Color}{InitState}{Input}{Output}{Execution}
			\test{F}{Name}{Type}{ManAut}{Color}{InitState}{Input}{Output}{Execution}


	\subsection{Tests for Non-Functional Requirements}

		\subsubsection{Look and Feel Requirements}
			% Appearance
			\test{N}{Aesthetic Similarity Check}{Dynamic}{Manual}{Black}{Generic State}{Users are asked to rate the aesthetic similarity between \textit{Rogue} and Rogue Reborn.}{A numeric quantity between 0 and 10, where 0 indicates that the graphics are entirely disjoint and 10 indicates that the graphics are virtually indistinguishable.}{A random sample of users will be asked to play \textit{Rogue} and the Rogue Reborn variant for \hyperref[symbolicParameters]{PLAYTEST\_SHORT\_TIME} minutes.  Afterwards, they will be asked to judge the graphical similarity of the games based on the aforementioned scale.}

		\subsubsection{Usability and Humanity Requirements}
			% Ease of Use
			\test{N}{Interest Gauge Check}{Dynamic}{Manual}{Black}{Generic State}{New users are instructed to play Rogue Reborn.}{The quantity of time the user willingly decides to play the game.}{A random sample of users who are unfamiliar with \textit{Rogue} will be asked to play Rogue Reborn until they feel bored (or \hyperref[symbolicParameters]{MAXIMUM\_ENTERTAINMENT\_TIME} has expired).  Once the user indicates that they are no longer interested in the game, their playing time will be recorded.}

			% Personalization and Internationalization
			\test{N}{English Mechanics Check}{Static}{Manual}{White}{Developer State}{Rogue Reborn source code.}{An approximation of the English spelling, punctuation, and grammar mistakes that are visible through the GUI.}{All strings in the Rogue Reborn source code will be concatenated with a newline delimiter and outputted to a text file.  A modern edition of Microsoft Word from ~\citep{MicrosoftWord} will be used to open this generated text file, and a developer will manually correct all of the indicated errors that are potentially associated with a GUI output.}

			% Learning
			\test{N}{Key Comfort Check}{Dynamic}{Manual}{Black}{Generic State}{Users are asked to rate the intuitiveness of the Rogue Reborn key bindings.}{A numeric quantity between 0 and 10, where 0 indicates that the key bindings are extremely confusing and 10 indicates that the key bindings are perfectly natural.}{A random sample of users who are inexperienced with the roguelike genre will be asked to play Rogue Reborn for \hyperref[symbolicParameters]{SHORT\_TIME} minutes without viewing the in-game help screen.  Next, the key bindings will be revealed, and the users will continue to play the game for an additional \hyperref[symbolicParameters]{PLAYTEST\_SHORT\_TIME} minutes.  Afterwards, they will be asked to judge the quality of the key bindings based on the aforementioned scale}

		\subsubsection{Performance Requirements}
			% Speed and Latency
			\test{N}{Response Delay Check}{Dynamic}{Automatic}{White}{Generic State}{Users are instructed to play Rogue Reborn.}{A log of occurrences that indicate events where a computation that was initiated by a user input took an excessive quantity of time to execute.}{A random sample of experienced users will be asked to play a special version of Rogue Reborn for \hyperref[symbolicParameters]{PLAYTEST\_MEDIUM\_RANGE} minutes.  This edition will utilize a StopWatch implementation to measure the execution time of a computation, and if the computation exceeds \hyperref[symbolicParameters]{RESPONSE\_SPEED} milliseconds, the user action and the associated timestamp will be recorded in a log file.}

			% Precision or Accuracy
			\test{N}{Overflow Avoidance Check}{Static}{Manual}{White}{Developer State}{Rogue Reborn source code.}{All declarations of integer-typed variables.}{All occurrences of lines that match \hyperref[symbolicParameters]{REGEX\_INTEGER} (i.e., integer declarations) in the Rogue Reborn source code will be outputted to a file.  A group of Rogue++ developers will then review these declarations together and alter them if deemed necessary to avoid integer overflow issues.}

			% Reliability or Availability
			\test{N}{Crash Collection Check}{Dynamic}{Manual}{Black}{Generic State}{Playtesters are instructed to play Rogue Reborn for at least \hyperref[symbolicParameters]{PLAYTEST\_LONG\_TIME} hours.}{A collection of crash occurrences along with a detailed description of the failure environment.}{All Rogue Reborn playtesters will be required to play the game for at least \hyperref[symbolicParameters]{PLAYTEST\_LONG\_TIME} hours in total (spanned over multiple sessions if desired).  Every time the application crashes, the playtester must record the incident along with a description of the visible GUI state and the steps required to reproduce the failure.  After this data has been collected, the Rogue++ team will address every crash occurrence by either resolving the issue or confidently declaring that the event is irreproducible.}

			% Capacity
			\test{N}{Score Overflow Check}{Dynamic}{Dynamic}{White}{High Score State}{A high score record file containing a large quantity of entries.}{Rogue Reborn GUI displaying the top high scores.}{The Rogue Reborn developers will artificially fabricate a high score record file with at least \hyperref[symbolicParameters]{HIGH\_SCORE\_CAPACITY} + 2 records.  The game will then be played until the high score screen is revealed; only the top \hyperref[symbolicParameters]{HIGH\_SCORE\_CAPACITY} scores should be displayed.}

		\subsubsection{Operational and Environment Requirements}
			% Expected Physical Environment
			\test{N}{Processor Compatibility Check}{Dynamic}{Manual}{Black}{Fresh State}{Users are instructed to install and run Rogue Reborn on their personal machines.}{An indication of whether or not the game is able to successfully execute.}{A random sample of users with computers that are equipped with Intel x64 processors will be asked to download the latest Rogue Reborn distribution, perform any necessary installation, and then run the executable file.  The user will then report if the game was able to successfully run on their machine.}

			% Productization
			\test{N}{Streamline Distribution Check}{Static}{Manual}{Black}{Developer State}{Rogue Reborn distribution package.}{An indication of whether or not the distribution contains any files aside from the primary executable and the associated development licenses.}{The public distribution package will be visually inspected for extraneous files.}

		\subsubsection{Maintainability Requirements}
			% Maintenance
			\test{N}{Bug Productivity Check}{Static}{Manual}{Black}{Developer State}{All ITS issues labeled as bugs in the Rogue Reborn GitLab repository.}{An indication of whether or not all bug reports were closed within a month of their conception.}{The Rogue Reborn GitLab repository will be queried for all issues concerning bugs (which are denoted by a ``Bug'' label).  Next, a developer will manually verify that every closed bug fix request was resolved within a month of its creation.}

			% Adaptability
			\test{N}{Linux Compatibility Check}{Dynamic}{Manual}{Black}{Fresh State}{Users are instructed to run Rogue Reborn on their personal machine.}{An indication of whether the game can successfully execute.}{A random sample of users with computers that use a modern 64-bit Linux operating system will be asked to download the latest Rogue Reborn distribution, perform any necessary installation, and then run the executable file.  The user will then report if the game was able to successfully run on their machine.}

		\subsubsection{Security Requirements}
			% Integrity
			\test{N}{Illegal Records Check}{Dynamic}{Manual}{White}{Seasoned State}{A corrupted high score record file.}{Rogue Reborn GUI displaying the top high scores.}{The Rogue++ team will illegally modify a high score record file by manually altering or adding values such that the expected format or value integrity is violated.  These modifications should include negative high score values, missing text, and incorrect delimiter usage.  The game will then be played until the high score screen is revealed; all invalid record file contents should be ignored and amended in the next write to the record file.}

		\subsubsection{Legal Requirements}
			% Compliance
			\test{N}{License Presence Check}{Static}{Manual}{Black}{Developer State}{Rogue Reborn distribution package.}{An indication of whether or not the distribution is missing any mandatory license files.}{The original \textit{Rogue} source code hosted by ~\citep{BSDRogue} will be reviewed for legal requirements, and the public distribution package will be visually inspected to ensure that all mandatory license files are present.}

		\subsubsection{Health and Safety Requirements}
			% Seizure Prevention
			\test{N}{Seizure Prevention Check}{Dynamic}{Manual}{Black}{Developer State}{Two screenshots denoting the largest possible luminosity difference present between consecutive frames.}{The difference in luminosity between the two captured frames.}{After identifying the frame pair that is most likely to induce a seizure, the game will be played to reach the states that reflect each frame (this should be a brief process; no clever game model manipulation is required).  At the occurrence of each desired frame, the game screen will be captured and saved.  At this point, the average monochrome luminance across each frame will be calculated according to the formula \[L = 0.299R + 0.587G + 0.114B\] where $L$ is the luminance, $R$ is the red RGB component, $G$ is the green RGB component, and $B$ is the blue RGB component ~\citep{MonochromeLuminance}.  Finally, the absolute value of the luminance difference can then compared to \hyperref[symbolicParameters]{LUMINOSITY\_DELTA}.}   

\newpage
\section{Tests for Proof of Concept}
\label{section4}

	\subsection{Static Testing}
	
		\test{P}{Compile Test}{Static}{Automatic}{White}{None}{Program Source}{Program Executable}{Verify that the program compiles with g++.}
		
		\test{P}{Memory Check}{Dynamic}{Manual}{White}{None}{A brief but complete playthrough of the game.}{Breakdown of program memory usage.}{A tester will briefly play the game, and a developer will use Valgrind's memcheck utility to verify that program does not leak memory or utilize uninitialized memory.}

	\subsection{Rendering}
		\test{P}{Render Check}{Dynamic}{Manual}l{Black}{Gameplay State}{30-60 seconds of gameplay.}{ The player character and any dungeon features should be shown at the correct location with the correct glyphs. Correct player statistics will be shown along the bottom. The dialog box will correctly display the log and any prompts.}{A tester will manually play the game and verify the display is correct.}

	\subsection{Dungeon Generation}
		
		\test{P}{Dungeon-Gen Check}{Dynamic}{Manual}{Black}{None}{Repeated restarts of the game}{Level should contain \hyperref[symbolicParameters]{ROOMS\_PER\_LEVEL} rooms, which should form a connected graph.}{A tester will manually start the game, briefly explore the level to verify correct generation, then repeat this process until confidence is achieved.}

	\subsection{Basic Movement}

		\test{P}{Movement Check}{Dynamic}{Manual}{Black}{Gameplay State}{Movement commands}{Player should move about the level, without clipping through walls, failing to walk through empty space, or jump to an unconnected square.}{A tester will manually walk through the level, and visually verify correctness.}

	\subsection{Score File}

		\test{P}{Scoring File Check}{Dynamic}{Manual}{Black}{Menu State}{Enter name, then quit, restart game, enter name again, and quit.}{1st name should appear in both the first and second score screens. The 2nd should appear in the second. Both should have correct values for level, cause of death/quit, and gold collected.}{A developer will manually perform the above input, and verify the output. Should be tested both with and without an initial score file.}

	\subsection{Line of Sight System}

		\test{P}{LoS Check}{Dynamic}{Manual}{Black}{Gameplay State}{Movement commands}{Screen should display correct portions of level, with correct coloration schemes. This means that the player should be able to see the entirety of a room they are in or in the doorway of, and \hyperref[symbolicParameters]{VIEW\_DISTANCE} squares away if they are in a corridor. Squares that the player has seen in the past but cannot see currently should be shown greyed out. Squares they have not seen should be black and featureless.}{A developer will manually walk through the level, verifying that the above LoS rules are preserved, especially in edge cases like the corners of rooms and doorways.}

\newpage
\section{Comparison to Existing Implementation}	
\label{section5}
%How does it compare to the original and how does that help us test it
%Monster damage, etc.
	
\newpage
\section{Unit Testing Plan}
\label{section6}
	
	After examining the boost library's utilities for unit testing, we have decided we will not use a unit testing framework for testing the product. We concluded that adding a framework would not make the work significantly easier, while reducing our flexibility and adding installation difficulties. Since we are not using a framework, drivers will be written by hand. Stubs will be produced when necessary to simulate system components. Since there are no database or network connections, stubs should hopefully be kept to a minimum. However, functions may be required to construct objects in states suitable for easy testing, for example creating a level or player with certain known properties, rather than by random generation.
	
	\subsection{Unit testing of internal functions}
		Internal functions in the product will be unit tested. This will be reserved for more complex functions so as to not waste development time unnecessarily. As complete code coverage is not a goal, generic code coverage metrics will not be used. Instead, care will be taken that complex functions are covered by unit tests. The following are examples of internal functions that are initial candidates for unit testing. Other functions will be added as necessary:
		\begin{itemize}
			\item The dungeon generation functions. The work of generating the dungeon is complex, but it is also easy to automate verification of dungeon properties such as a correct number of rooms, connectedness, compliance with formulas for item generation, presence or absence of certain key features such as the stairs connecting levels or the Amulet of Yendor in the final level.
			\item The keyboard input functions. As libtcod provides a Key struct which models keyboard input, we can mock/automate these functions. They are fairly complex, and since they return a pointer to the next desired state (similar to a finite state machine) we can easily verify their behavior.
			\item The item activation functions. For example it could be verified that when the player drank a potion of healing their health increased (if it was not at its maximum), that a scroll of magic-mapping is reveals the level, or that a scroll of identification reveals the nature of an item.
			\item The item storage functions. Each item is mapped to a persistent hotkey in the player's inventory. Certain items can stack with copies, reducing the amount of inventory space they take up, and how they are displayed. These factors make the inventory fairly complex. It is however easily verifiable, and automated testing can examine edge cases that would be impractical to test manually.
		\end{itemize}

	\subsection{Unit testing of output files}
		There is only one output file for the product, the high score file, which stores the scores in a csv format. The production and reading of this file can be unit-tested by verifying its contents after writing to it, and by providing a testing version of the file with known contents and verifying the function reads them correctly.

\newpage

\bibliographystyle{plainnat}

\bibliography{TestPlan}

\newpage
\section{Appendix}
\label{section7}

	This is where you can place additional information.

	\subsection{Symbolic Parameters}

		\begin{table}[h!]
			\centering
			\caption{\textbf{Symbolic Parameter Table}}
			\label{symbolicParameters}
			\bigskip
			\def\arraystretch{1.6}


			\begin{tabular}{| c | c |}
				\bottomrule
				\textbf{Parameter} & \textbf{Value} \\
				\hline
				ROOMS\_PER\_LEVEL & 9 \\
				FINAL\_LEVEL & 26 \\
				HEIGHT\_RESOLUTION & 400 \\
				LUMINOSITY\_DELTA & 0.5 \\
				MINIMUM\_ENTERTAINMENT\_TIME & 20 \\
				MINIMUM\_RESPONSE\_SPEED & 30 \\
				HIGH\_SCORE\_CAPACITY & 15 \\
				PLAYTEST\_SHORT\_TIME & 5 \\
				PLAYTEST\_MEDIUM\_RANGE & 10-20 \\
				PLAYTEST\_LONG\_TIME & 3 \\
				REGEX\_INTEGER & \lstinline$(char|int|long).*(,|;)$ \\
				START\_LEVEL & 1 \\
				VIEW\_DISTANCE & 1 \\
				WIDTH\_RESOLUTION & 1280 \\
				\toprule
			\end{tabular}
		\end{table}

\newpage
\subsection{Usability Survey Questions}

	\begin{enumerate}
		\item Is there any game feature you were unable to figure out how to utilize?
		\item How helpful was the help screen for you?
		\item Was there anything going on in the game that the interface failed to make clear to you or deceived you about?
		\item What common UI interactions did you find particularly lengthy?
		\item What aspects of the interface did you find unintuitive?
		\item How responsive was the interface?
		\item How easy was it to see everything that was going on?
		\item How effective are the graphics/symbols?
		\item Would an alternative input device such as a mouse make interacting with the interface easier for you?
		\item Is there any extra functionality you would like added to the interface?
		\item How difficult was it to learn the game? How much experience do you have with Roguelikes?
		\item How helpful was the original game manual?
		\item How pleasing was the color scheme?
		\item Was the font large enough for easy use?
		\item Were you able to learn the hotkeys easily?
	\end{enumerate}

\end{document}
