\documentclass[12pt, titlepage]{article}

% Packages

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage[round]{natbib}
\usepackage[usenames, dvipsnames]{color}
\usepackage{tikz}

% Setup

\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=ForestGreen,
    linkcolor=MidnightBlue,
    urlcolor=blue
}

\lstset{
	basicstyle=\ttfamily\footnotesize
}

% Custom Commands

\newcounter{funCounter}
\newcounter{nonCounter}
\newcounter{pocCounter}

\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0em}}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0em}}p{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0em}}p{#1}}

\newcommand\testTitle{NULL}
\newcommand\testCounter{NULL}

% Arguments
%    1 = {F, N, P} for functional, non-functional, and PoC respectively
%    2 = {Static, Dynamic}
%    3 = {Manual, Automatic}
%    4 = {Black, White}
%    5 = Initial state
%    6 = Input
%    7 = Output
%    8 = How the test will be performed

\newcommand{\test}[8]{	
	\begin{table}[H]
		\if#1F
			\stepcounter{funCounter}
			\renewcommand\testTitle{Functional }
			\renewcommand\testCounter{\thefunCounter} 
		\else
			\if#1N
				\stepcounter{nonCounter}
				\renewcommand\testTitle{Non-Functional }
				\renewcommand\testCounter{\thenonCounter} 
			\else
				\stepcounter{pocCounter}
				\renewcommand\testTitle{Proof of Concept }
				\renewcommand\testCounter{\thepocCounter} 
			\fi
		\fi
		
		\centering
		\def\arraystretch{1.6}
		\begin{tabular}{| R{7em} L{25em} |}
			\bottomrule
			\multicolumn{2}{| c |}{\textbf{\testTitle Test} \# \testCounter} \\
			\hline
			\textit{Type:} & #2 / #3 / #4 Box\\
			\textit{Initial State:} & #5 \\
			\textit{Input:} & #6 \\
			\textit{Output:} & #7 \\
			\textit{Execution:} & #8 \\
			\toprule
		\end{tabular}
	\end{table}
	\smallskip
}

% Title Page Elements

\title{SE 3XA3: Test Plan\\Rogue Reborn}

\author{Group \#6, Team Rogue++\\\\
	\begin{tabular} {l r}
		Ian Prins & prinsij \\
		Mikhail Andrenkov & andrem5 \\
		Or Almog & almogo
	\end{tabular}
}

\date{Due Monday, October 31\textsuperscript{st}, 2016}

\input{../Comments}

\begin{document}

\maketitle

% Report outline

\pagenumbering{roman}
\tableofcontents
\listoftables
\listoffigures

% Revision Table

\begin{table}[bp!]
	\caption{\bf Revision History}
	\bigskip
	\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
		\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
		\midrule
		10/21/16 & 0.0 & Initial Setup\\
		10/24/16 & 0.1 & Added Unit Testing and Usability Survey \\
		10/24/16 & 0.2 & Added Most of Section 2 \\
		10/24/16 & 0.3 & Added Section 1 \\
		10/26/16 & 0.4 & Added PoC tests \\
		10/26/16 & 0.4.1 & Added Test Template \\
		10/30/16 & 0.5 & Added Non-Functional Req. Tests \\
		10/30/16 & 0.5.1 & Added Bibliography \\
		10/31/16 & 0.6 & Switch PoC to test template \\
		\bottomrule
	\end{tabularx}
\end{table}

\newpage

% Report Content

\pagenumbering{arabic}

\section{General Information}
\label{section1}

	\subsection{Purpose}
		The purpose of this document is to explore the verification process that will be applied to the Rogue Reborn project.  After reviewing the document, the reader should understand the strategy, focus, and motivation behind the efforts of the Rogue++ testing team.   

	\subsection{Scope}
		This report will encompass all technical aspects of the testing environment and implementation plan, as well as other elements in the domain of team coordination and project deadlines.  The document will also strive to be comprehensive by providing context behind critical decisions, motivating the inclusion of particular features by referring to the existing \textit{Rogue} implementation, and offering a large variety of tests for various purposes and hierarchical units.  Aside from the implementation, the report will also discuss a relevant component from the requirements elicitation process.

	\subsection{Acronyms, Abbreviations, and States}
		
		\begin{table}[H]
			\centering
			\caption{\textbf{Table of Abbreviations and Acronyms}}
			\label{TableAbbreviations}
			\bigskip
			\begin{tabularx}{\textwidth}{p{3cm}X}
				\toprule
				\textbf{Abbreviation} & \textbf{Definition} \\
				\midrule
				GUI & Graphical User Interface\\
				PoC & Proof of Concept\\
				\bottomrule
			\end{tabularx}
		\end{table}

		\begin{table}[H]
			\centering
			\caption{\textbf{Table of Definitions}}
			\label{TableDefinitions}
			\bigskip
			\begin{tabularx}{\textwidth}{p{3.5cm}X}
				\toprule
				\textbf{Term} & \textbf{Definition}\\
				\midrule
				\textbf{Boost} & C++ utility library that includes a comprehensive unit testing framework\\
				\textbf{Frame} & An instantaneous ``Snapshot'' of the GUI screen\\
				\textbf{Libtcod} & Graphics library that specializes in emulating a roguelike experience\\
				\textbf{Monochrome Luminance} & The brightness of a given colour (with respect to the average sensitivity of the human eye)\\
				\textbf{Permadeath} & Feature of roguelike games whereby a character death will end the game\\
				\textbf{Roguelike} & Genre of video games characterized by ASCII graphics, procedurally-generated levels, and permadeath\\
				\bottomrule
			\end{tabularx}
		\end{table}	

		\begin{table}[H]
			\centering
			\caption{\textbf{Table of States}}
			\label{StateDefinitions}
			\bigskip
			\begin{tabularx}{\textwidth}{p{3.5cm}X}
				\toprule
				\textbf{State} & \textbf{Definition}\\
				\midrule
				\textbf{Developer State} & The file system state corresponding to the latest source code revision from the Git repository\\
				\textbf{Fresh State} & The file system state corresponding to a ``fresh'' Rogue Reborn installation\\
				\textbf{Gameplay State} & Any application state that reflects the actual gameplay\\
				\textbf{High Score State} & Any application state that reflects the top high scores screen\\
				\textbf{Menu State} & Any application state that reflects the opening menu\\
				\textbf{Public Test State} & The system state corresponding to an installation of Rogue Reborn that is shared by a subset of the public game testers\\
				\bottomrule
			\end{tabularx}
		\end{table}	

	\subsection{Overview of Document}
		The early sections of the report will describe the testing environment and the logistic components of the Rogue Reborn testing effort, including the schedule and work allocation.  Next, a suite of tests will be discussed with respect to the functional requirements, nonfunctional requirements, and proof of concept demonstration.  Upon discussing the relevance of this project to the original \textit{Rogue}, a variety of unit tests will be given followed by a sample usability survey to guage the interest and opinion of the Rogue Reborn game.  A breakdown of the sections is listed below:

		\begin{itemize}
			\item \hyperref[section1]{\S 1} Brief overview of the report contents
			\item \hyperref[section2]{\S 2} Project logistics and the software testing environment
			\item \hyperref[section3]{\S 3} Description of system-level integration tests (based on requirements)
			\item \hyperref[section4]{\S 4} Explanation of test plans that were inspired by the PoC demonstration
			\item \hyperref[section5]{\S 5} Comparison of the existing \textit{Rogue} to the current project in the context of testing
			\item \hyperref[section6]{\S 6} Outline of the module-level unit tests 
			\item \hyperref[section7]{\S 7} Appendix for symbolic parameters and the aforementioned usability survey
		\end{itemize}

\newpage
\section{Plan}
\label{section2}
		
	\subsection{Software Description}

	Initially, the plan for testing involved the usage of a pre-made testing system called Boost. Boost has industry renown and is very well documented. The drawback to using such a profound system is exactly its advantage - it is heavy, globally encompassing, and requires a lot of work to use properly. The Boost library is suitable for projects spanning years, with dedicated testing teams. This is not the present situation. With hardly over a month until the completion of the project, starting to use Boost would be most unwise.\\

	Instead, an alternative solution has been proposed and implemented. Native test cases can be written in C++ to do exactly that which is required. The details of this implementation will be explained in the parts to follow.

	\subsection{Test Team}

	All members of the team will take part in the testing procedure. While Mikhail was given the title of project manager, and Ian C++ expert, Ori was assigned the role of testing expert. Testing will be monitored by Ori, but of course every member of the team will contribute to the testing facilities. It would be desirable for the team member who wrote class $C$ to write the unit tests for this class. Due to the dependency-tree-like structure of the project's design, there will be cases where a unit test for one class encompasses a partial system test for another one. This can be extrapolated from the class inheritance diagram.

	\subsection{Automated Testing Approach}

	We have made a very large attempt at automating whatever we could for this project. In the real world, any task that \textit{can} be automated, is automated. The steps we have taken are as follows:

	\begin{itemize}
		\item Set up a GitLab pipeline for the project. The pipeline is programmed to run a series of commands on an external VPS whenever a push is made to the git repository. Each run is documented and its history may be accessed.
		\item Write a special makefile that outputs 2 executables: the first being the actual project, and the second the project's tests. The details will be delved into in the following sub-section.
		\item The team's primary method of communication is Slack, a cross-platform, programmer-friendly chat interface. We hooked up the GitLab project repository to the Slack channel such that whenever a push is made or an issue addressed, a notification is sent. This method makes it far easier to communicate about project-related inquiries.
	\end{itemize}

	\subsection{Testing Tools}

	The special makefile discussed previously utilizes a phenomenon of C++ to perform the necessary steps. First, it places \textit{all} source files into a dedicated folder, distinguishing between program files and test files. This is an absolutely necessary step, as there is an important relationship between \textit{source} and \textit{test} classes. See the diagram below:

	\bigskip
	\bigskip
	\bigskip

	% Venn-diagram
	\begin{center}
		\begin{tikzpicture}[fill=white]
			% left hand
			\scope
			\clip (1,0) circle (1);
			\endscope

			% right hand
			\scope
			\clip (0,0) circle (1);
			\endscope

			% outline
			\draw 	(-1,0) circle (2) (-4,1)  node [text=black,above] {$Source$}
			 		(1,0) circle (2) (4,1)  node [text=black,above] {$Test$};
		\end{tikzpicture}
	\end{center}

	\bigskip
	\bigskip

	As the diagram above depicts, there are classes shared between both final programs. The vast majority of classes fall in the center, required by both the final project and its testing component. The files required by the test which are not required by the source are, obviously, testing-related files. These are the files that contain the test case implementations. At the time of writing, there is actually only one file required by source that is not required by the test, and that is the source program entry (i.e. the file that contains the main() method).

	\bigskip

	The entire procedure of file collection, compilation, and separate linking is handled by the makefile, and is triggered by the "make" command. Then, simply running Test.exe will fire off all of the pre-written tests.

	\bigskip

	There is a plan to implement a python script on the GitLab pipeline that will cause the build to fail if any of the tests do not pass. At the time of writing this document, it is not yet implemented, but note will be made when it does. It should be noted that if a build fails, the pipeline not only reports the failure, but also logs where the failure happened, down to the specific test case. This will hopefully make debugging a more pleasant experience later on.

	\bigskip

	As an extra safety measure, the Rogue++ team will also be utilizing a tool called Valgrind in the testing procedure. Valgrind is a tool that tests the amount of memory a C++ program utilizes, and detects errors such as memory leaks. This is an extremely useful and powerful tool. C++, unlike Java and other high level languages, does not have a built-in garbage collector. Garbage collectors are to be implemented by the user. Due to this, it is easy to accidentally leave behind an object or two, causing a memory leak in the program. Valgrind detects this, and will provide useful information during development and testing.

	\subsection{Testing Schedule}
		
	See Gantt Chart at the following url ... TODO

\newpage
\section{System Test Description}
\label{section3}
	
	\subsection{Tests for Functional Requirements}

		\subsubsection{Area of Testing1}
		
		\test{F}{Dynamic}{Manual}{Black}{Menu screen}{Alphanumeric keyboard interrupts}{Valid characters that are pressed will appear on the screen beside the character name prompt}{Typing the name of the character on the keyboard}

	\subsubsection{Area of Testing2}

	\subsection{Tests for Non-Functional Requirements}

		\subsubsection{Look and Feel Requirements}
			% Appearance
			\test{N}{Dynamic}{Manual}{Black}{Public Test State}{Users are asked to rate the aesthestic similarity between \textit{Rogue} and Rogue Reborn.}{A numeric quantity between 0 and 10, where 0 indicates that the graphics are entirely disjoint and 10 indicates that the graphics are virtually indistinguishable.}{A random sample of users will be asked to play \textit{Rogue} and the Rogue Reborn variant for \hyperref[symbolicParameters]{PLAYTEST\_SHORT\_TIME} minutes a piece.  Afterwards, they will be asked to judge the graphical similarity of the games based on the aforementioned scale.}
		\subsubsection{Usability and Humanity Requirements}
			% Ease of Use
			\test{N}{Dynamic}{Manual}{Black}{Public Test State}{New users are instructed to play Rogue Reborn.}{The quantity of time the user willingly decides to play the game.}{A random sample of users who are unfamiliar with \textit{Rogue} will be asked to play Rogue Reborn until they feel bored (or \hyperref[symbolicParameters]{MAXIMUM\_ENTERTAINMENT\_TIME} has expired).  Once they indicate that they no longer wish to play, their playing time will be recorded.}
			% Personalization and Internationalization
			\test{N}{Static}{Manual}{White}{Developer State}{Rogue Reborn source code.}{An approximation of the English spelling, punctuation, and grammar mistakes that can be visible from the GUI.}{All strings in the Rogue Reborn source code will be concatenated with a newline delimiter and placed in a text file.  A modern edition of Microsoft Word will be used to open this generated text file, and a developer can then manually correct all indicated errors that are potentially associated with a GUI output.}
			% Learning
			\test{N}{Dynamic}{Manual}{Black}{Public Test State}{Users are asked to rate the intuitiveness of the Rogue Reborn key bindings.}{A numeric quantity between 0 and 10, where 0 indicates that the key bindings are extremely confusing and 10 indicates that the key bindings are perfectly natural.}{A random sample of users who are inexperienced with the roguelike genre will be asked to play Rogue Reborn for \hyperref[symbolicParameters]{SHORT\_TIME} minutes without viewing the key binding help screen.  Next, the key bindings will be revealed and the users will continue to play for an additional \hyperref[symbolicParameters]{PLAYTEST\_SHORT\_TIME} minutes.  Afterwards, they will be asked to judge the quality of the key bindings based on the aforementioned scale}
		\subsubsection{Performance Requirements}
			% Speed and Latency
			\test{N}{Dynamic}{Automatic}{White}{Public Test State}{Users are instructed to play Rogue Reborn.}{A log of occurrences where a computation that was initiated by a user input took an excessive quantity of time to execute.}{A random sample of experienced users will be asked to play a special version of Rogue Reborn for \hyperref[symbolicParameters]{PLAYTEST\_MEDIUM\_RANGE} minutes.  This version will use a StopWatch implementation to measure the execution time of a computation, and if such a computation exceeds \hyperref[symbolicParameters]{RESPONSE\_SPEED} milliseconds, the user action and timestamp will be recorded in a log file.}
			% Precision or Accuracy
			\test{N}{Static}{Manual}{White}{Developer State}{Rogue Reborn source code.}{All declarations of integer-typed variables.}{A recursive \lstinline$grep$ command will be used to capture all lines in the Rogue Reborn source code that match \hyperref[symbolicParameters]{REGEX\_INTEGER} (i.e., integer declarations).  A group of Rogue++ developers can review these declarations together and alter them if deemed necessary to avoid integer overflow issues.}
			% Reliability or Availability
			\test{N}{Dynamic}{Manual}{Black}{Public Test State}{Playtesters are instructed to play Rogue Reborn for at least \hyperref[symbolicParameters]{PLAYTEST\_LONG\_TIME} hours.}{A collection of crash occurrences along with descriptions that explain how the failure occurred.}{All Rogue Reborn playtesters will be required to play the game for at least \hyperref[symbolicParameters]{PLAYTEST\_LONG\_TIME} hours in total (spanned over multiple sessions if desired).  If the application crashes during any time, the user must record the incident along with a description of the visible GUI state and the steps required to reproduce the failure.  The Rogue++ team must address each crash by either resolving the issue or confidently declaring that the event is irreproducible.}
			% Capacity
			\test{N}{Dynamic}{Manual}{White}{High Score State}{A high score record file containing a large quantity of entries.}{Screen denoting the top high scores.}{The Rogue Reborn developers will artificially fabricate a high score record file with at least \hyperref[symbolicParameters]{HIGH\_SCORE\_CAPACITY} + 2 records.  One round of the game will be played, and when the high score screen is revealed, only the top \hyperref[symbolicParameters]{HIGH\_SCORE\_CAPACITY} scores should be displayed.}
		\subsubsection{Operational and Environment Requirements}
			% Expected Physical Environment
			\test{N}{Dynamic}{Manual}{Black}{Fresh State}{Users are instructed to run Rogue Reborn on their personal machine.}{An indication of whether the game can successfully execute.}{A random sample of users with computers that are equipped with Intel x64 processors will be asked to download the latest Rogue Reborn distribution and attempt to run the executable.  The user will then report if the game successfully runs on their machine.}
			% Productization
			\test{N}{Static}{Manual}{Black}{Developer State}{Rogue Reborn distribution package.}{An indication of whether or not the distribution contains any files aside from the primary executable and the associated licenses.}{The public distribution package will be visually inspected for extraneous files.}
		\subsubsection{Maintainability Requirements}
			% Maintenance
			\test{N}{Static}{Manual}{Black}{Developer State}{All ITS issues labeled as bugs in the Rogue Reborn GitLab repository.}{A list of all bug reports and their corresponding resolution date (if closed).}{The Rogue Reborn GitLab repository will be queried for all issues concerning bugs (which are denoted by a ``Bug'' label).  A developer can then manually verify that every closed bug fix request was resolved within a month of its creation.}
			% Adaptability
			\test{N}{Dynamic}{Manual}{Black}{Fresh State}{Users are instructed to run Rogue Reborn on their personal machine.}{An indication of whether the game can successfully execute.}{A random sample of users with computers that use a modern 64-bit Linux operating system will be asked to download the latest Rogue Reborn distribution and attempt to run the executable.  The user will then report if the game successfully runs on their machine.}
		\subsubsection{Security Requirements}
			% Integrity
			\test{N}{Dynamic}{Manual}{White}{High Score State}{A corrupted high score record file.}{Screen denoting the top \hyperref[symbolicParameters]{HIGH\_SCORE\_CAPACITY} (valid) high scores.}{The Rogue++ team will illegally modify a high score record file by manually altering or adding values such that the expected format or value integrity is violated.  These modifications should include negative high score values, missing text, and incorrect delimiter usage.  The game will then be executed to reach the High Score State, where invalid record file contents should be ignored and amended in the next write to the file.}
		\subsubsection{Legal Requirements}
			% Compliance
			\test{N}{Static}{Manual}{Black}{Developer State}{Rogue Reborn distribution package.}{An indication of whether or not the distribution is missing any mandatory license files.}{The original \textit{Rogue} source code (as referenced on the Rogue Reborn GitLab homepage) will be reviewed for legal requirements, and the public distribution package will be visually inspected to ensure that all license files are present.}
		\subsubsection{Health and Safety Requirements}
			\test{N}{Static}{Manual}{Black}{Developer State}{Two screenshots denoting the largest possible luminosity difference present between two consecutive frames.}{The difference in luminosity between the two captured frames.}{After identifying the frame pair that is most likely to induce a seizure, the game will be played to reach the states that reflect each frame (this should be a brief process; no clever game model manipulation is required).  At the occurrence of each desired frame, the game screen will be captured.  At this point, the average monochrome luminance across each frame will be calculated according to \[L = 0.299R + 0.587G + 0.114B\] where $L$ is the luminance, $R$ is the red RGB component, $G$ is the green RGB component, and $B$ is the blue RGB component ~\citep{MonochromeLuminance}.  Finally, the absolute value of the luminance difference can then compared to \hyperref[symbolicParameters]{LUMINOSITY\_DELTA}.}   

\newpage
\section{Tests for Proof of Concept}
\label{section4}

	\subsection{Static Testing}
	
		\test{P}{Static}{Automatic}{White}{None}{Program Source}{Program Executable}{Verify that the program compiles with g++.}
		
		\test{P}{Dynamic}{Manual}{White}{None}{A brief but complete playthrough of the game.}{Breakdown of program memory usage.}{A tester will briefly play the game, and a developer will use valgrind's memcheck utility to verify that program does not leak memory or utilize uninitialized memory.}

	\subsection{Rendering}
		\test{P}{Dynamic}{Manua}l{Black}{Gameplay State}{30-60 seconds of gameplay.}{ The player character and any dungeon features should be shown at the correct location with the correct glyphs. Correct player statistics will be shown along the bottom. The dialog box will correctly display the log and any prompts.}{A tester will manually play the game and verify the display is correct.}

	\subsection{Dungeon Generation}
		
		\test{P}{Dynamic}{Manual}{Black}{None}{Repeated restarts of the game}{Level should contain ROOMS\-PER\_LEVEL rooms, which should form a connected graph.}{A tester will manually start the game, briefly explore the level to verify correct generation, then repeat this process until confidence is achieved.}

	\subsection{Basic Movement}

		\test{P}{Dynamic}{Manual}{Black}{Gameplay State}{Movement commands}{Player should move about the level, without clipping through walls, failing to walk through empty space, or jump to an unconnected square.}{A tester will manually walk through the level, and visually verify correctness.}

	\subsection{Score File}

		\test{P}{Dynamic}{Manual}{Black}{Menu State}{Enter name, then quit, restart game, enter name again, and quit.}{1st name should appear in both the first and second score screens. The 2nd should appear in the second. Both should have correct values for level, cause of death/quit, and gold collected.}{A developer will manually perform the above input, and verify the output. Should be tested both with and without an initial score file.}

	\subsection{Line of Sight System}

		\test{P}{Dynamic}{Manual}{Black}{Gameplay State}{Movement commands}{Screen should display correct portions of level, with correct coloration schemes. This means that the player should be able to see the entirety of a room they are in or in the doorway of, and VIEW\_DISTANCE squares away if they are in a corridor. Squares that the player has seen in the past but cannot see currently should be shown greyed out. Squares they have not seen should be black and featureless.}{A developer will manually walk through the level, verifying that the above LoS rules are preserved, especially in edge cases like the corners of rooms and doorways.}

\newpage
\section{Comparison to Existing Implementation}	
\label{section5}
	
\newpage
\section{Unit Testing Plan}
\label{section6}
	
	After examining the boost library's utilities for unit testing, we have decided we will not use a unit testing framework for testing the product. We concluded that adding a framework would not make the work significantly easier, while reducing our flexibility and adding installation difficulties. Since we are not using a framework, drivers will be written by hand. Stubs will be produced when necessary to simulate system components. Since there are no database or network connections, stubs should hopefully be kept to a minimum. However, functions may be required to construct objects in states suitable for easy testing, for example creating a level or player with certain known properties, rather than by random generation.
	
	\subsection{Unit testing of internal functions}
		Internal functions in the product will be unit tested. This will be reserved for more complex functions so as to not waste development time unnecessarily. As complete code coverage is not a goal, generic code coverage metrics will not be used. Instead, care will be taken that complex functions are covered by unit tests. The following are examples of internal functions that are initial candidates for unit testing. Other functions will be added as necessary:
		\begin{itemize}
			\item The dungeon generation functions. The work of generating the dungeon is complex, but it is also easy to automate verification of dungeon properties such as a correct number of rooms, connectness, compliance with formulas for item generation, presence or absence of certain key features such as the stairs connecting levels or the Amulet of Yendor in the final level.
			\item The keyboard input functions. As libtcod provides a Key struct which models keyboard input, we can mock/automate these functions. They are fairly complex, and since they return a pointer to the next desired state (similar to a finite state machine) we can easily verify their behavior.
			\item The item activation functions. For example it could be verified that when the player drank a potion of healing their health increased (if it was not at its maximum), that a scroll of magic-mapping is reveals the level, or that a scroll of identification reveals the nature of an item.
			\item The item storage functions. Each item is mapped to a persistent hotkey in the player's inventory. Certain items can stack with copies, reducing the amount of inventory space they take up, and how they are displayed. These factors make the inventory fairly complex. It is however easily verifiable, and automated testing can examine edge cases that would be impracticle to test manually.
		\end{itemize}

	\subsection{Unit testing of output files}
		There is only one output file for the product, the high score file, which stores the scores in a csv format. The production and reading of this file can be unit-tested by verifying its contents after writing to it, and by providing a testing version of the file with known contents and verifying the function reads them correctly.

\newpage

\bibliographystyle{plainnat}

\bibliography{TestPlan}

\newpage
\section{Appendix}
\label{section7}

	This is where you can place additional information.

	\subsection{Symbolic Parameters}

		\begin{table}[h!]
			\centering
			\caption{\textbf{Symbolic Parameter Table}}
			\label{symbolicParameters}
			\bigskip
			\def\arraystretch{1.6}


			\begin{tabular}{| c | c |}
				\bottomrule
				\textbf{Parameter} & \textbf{Value} \\
				\hline
				ROOMS\_PER\_LEVEL & 9 \\
				FINAL\_LEVEL & 26 \\
				HEIGHT\_RESOLUTION & 400 \\
				LUMINOSITY\_DELTA & 0.5 \\
				MINIMUM\_ENTERTAINMENT\_TIME & 20 \\
				MINIMUM\_RESPONSE\_SPEED & 30 \\
				HIGH\_SCORE\_CAPACITY & 15 \\
				PLAYTEST\_SHORT\_TIME & 5 \\
				PLAYTEST\_MEDIUM\_RANGE & 10-20 \\
				PLAYTEST\_LONG\_TIME & 3 \\
				REGEX\_INTEGER & \lstinline$(char|int|long).*(,|;)$ \\
				START\_LEVEL & 1 \\
				VIEW\_DISTANCE & 1 \\
				WIDTH\_RESOLUTION & 1280 \\
				\toprule
			\end{tabular}
		\end{table}

\newpage
\subsection{Usability Survey Questions}

	\begin{enumerate}
		\item Is there any game feature you were unable to figure out how to utilize?
		\item How helpful was the help screen for you?
		\item Was there anything going on in the game that the interface failed to make clear to you or deceived you about?
		\item What common UI interactions did you find particularly lengthy?
		\item What aspects of the interface did you find unintuitive?
		\item How responsive was the interface?
		\item How easy was it to see everything that was going on?
		\item How effective are the graphics/symbols?
		\item Would an alternative input device such as a mouse make interacting with the interface easier for you?
		\item Is there any extra functionality you would like added to the interface?
		\item How difficult was it to learn the game? How much experience do you have with Roguelikes?
		\item How helpful was the original game manual?
		\item How pleasing was the color scheme?
		\item Was the font large enough for easy use?
		\item Were you able to learn the hotkeys easily?
	\end{enumerate}

\end{document}
